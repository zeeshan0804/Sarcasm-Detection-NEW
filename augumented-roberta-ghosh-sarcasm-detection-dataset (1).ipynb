{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10363593,"sourceType":"datasetVersion","datasetId":6418721},{"sourceId":10363604,"sourceType":"datasetVersion","datasetId":6418730},{"sourceId":10363612,"sourceType":"datasetVersion","datasetId":6418738},{"sourceId":10363622,"sourceType":"datasetVersion","datasetId":6418748},{"sourceId":10363633,"sourceType":"datasetVersion","datasetId":6418756},{"sourceId":10363963,"sourceType":"datasetVersion","datasetId":6418999},{"sourceId":10370030,"sourceType":"datasetVersion","datasetId":6423277},{"sourceId":218031,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":185934,"modelId":208053}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":7450.863728,"end_time":"2025-01-18T17:21:08.867356","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-01-18T15:16:58.003628","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"02780c5ff73e472c88e127984e2357fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_11ba5be3a1cd405591f4027f69a9c150","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b1b9f168fbe745269135c979ae4c6b8d","tabbable":null,"tooltip":null,"value":456318}},"0913c3e7286f4b508c80510346313743":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"0c6df18a309b4f3792f79d130c2c4f1f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"0f48f59e911b47fea0cf9a763b448260":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"11ba5be3a1cd405591f4027f69a9c150":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19783d1739ef4c4c9099e7716d064aff":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"279a5eb2588547c1b6295168081a86b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"2f7393ef14f24eb8923a2ce478403e89":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_e680fc104e564811a9326636de1add92","placeholder":"​","style":"IPY_MODEL_e14c86265ec2455ab90cbcbfd1c7b1d8","tabbable":null,"tooltip":null,"value":" 25.0/25.0 [00:00&lt;00:00, 2.75kB/s]"}},"307561ea9a1d4a5db03f32b52e01d662":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_a7f5f15d81f44615ad78210901006abd","placeholder":"​","style":"IPY_MODEL_0f48f59e911b47fea0cf9a763b448260","tabbable":null,"tooltip":null,"value":" 499M/499M [00:20&lt;00:00, 26.7MB/s]"}},"355b710f9a58449f84bb724fb8a13fcf":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"35873a083d054258bd7499731044af57":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_8676d424980942e6a976f3a75f041f7c","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_355b710f9a58449f84bb724fb8a13fcf","tabbable":null,"tooltip":null,"value":25}},"3a6c4520a5734c2e8438edd77d81c516":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f40f2bfef8640a18909e77960968817":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"42ab82dc67f649edbddd2e8913e5f74a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"436e602727ec45dca3f88073dd2fa44c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_a9f8196ea5af45a18d8564411de39f46","placeholder":"​","style":"IPY_MODEL_b8abcd33c5e348249b7d237949e9ba06","tabbable":null,"tooltip":null,"value":" 899k/899k [00:00&lt;00:00, 1.50MB/s]"}},"441fa009d62a48078922cae7284c90f3":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_65d0984362a14c28a8c38f9553923d52","placeholder":"​","style":"IPY_MODEL_fa0512ec1149487691065beaeaf7b6d1","tabbable":null,"tooltip":null,"value":"tokenizer.json: 100%"}},"45b8f5cbfde444ffaad1e0419edbd79c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_441fa009d62a48078922cae7284c90f3","IPY_MODEL_8576a14653214daabe3779163f01c1de","IPY_MODEL_e38fbd50853a4594add57a67c7e74f75"],"layout":"IPY_MODEL_646631462a434e72a719f5e62f435714","tabbable":null,"tooltip":null}},"48d5682cee7d48b994437445d4040375":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_f7dcd8c00b1c433686b7f4b5d5f46243","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6db14fd5e311426c80b8883f98d153bb","tabbable":null,"tooltip":null,"value":898823}},"4cb3e3f92616421b9a8c7aa9e4433c7f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c18dea3328aa41709d757712b1a4a9ad","IPY_MODEL_c5678c1987ae403b84bc41cd8a92d5e6","IPY_MODEL_c3b5fc2096a248f6aba5adfc4b311696"],"layout":"IPY_MODEL_b08f56664fba493c80962b44a7df09ff","tabbable":null,"tooltip":null}},"54d53354e60b4607af55ab97a67a9e9a":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bc44718073f4b31a8f2b3cd3cfc6dee":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6058afeeed6a445c8a5568ba40400542":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e84caa896d24468589215e139d639568","IPY_MODEL_a904b5eaeab841bf8aeb10a4a79a5725","IPY_MODEL_307561ea9a1d4a5db03f32b52e01d662"],"layout":"IPY_MODEL_e247d23083cd4f03b369b4f8ecc17847","tabbable":null,"tooltip":null}},"646631462a434e72a719f5e62f435714":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65d0984362a14c28a8c38f9553923d52":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6db14fd5e311426c80b8883f98d153bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"722ffb73aac64935ba9b3e818563319b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82e6ba6ea91e4e2ba536ae0f7484f847":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fbfae1778f154d058d0ed632f581852d","IPY_MODEL_48d5682cee7d48b994437445d4040375","IPY_MODEL_436e602727ec45dca3f88073dd2fa44c"],"layout":"IPY_MODEL_a03151e35fbb4310850cccde820d80ce","tabbable":null,"tooltip":null}},"847f82addeb54586a493704de6afa8f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"8576a14653214daabe3779163f01c1de":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_923a7accea764e28b1d3a4ccee9c3d62","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d5af7e751cfe4f439c5168b99a3f303c","tabbable":null,"tooltip":null,"value":1355863}},"8676d424980942e6a976f3a75f041f7c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"923a7accea764e28b1d3a4ccee9c3d62":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94f8d9af130941449fb43eed21934e31":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_5bc44718073f4b31a8f2b3cd3cfc6dee","placeholder":"​","style":"IPY_MODEL_279a5eb2588547c1b6295168081a86b1","tabbable":null,"tooltip":null,"value":"merges.txt: 100%"}},"986abfab1b3049dfb41759fea2d09462":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"9d4e0782654946768a414ad6251b01aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"a03151e35fbb4310850cccde820d80ce":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a18d732a0c2c4e979116cbb75e64055b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7f5f15d81f44615ad78210901006abd":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a904b5eaeab841bf8aeb10a4a79a5725":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_f684dc9efc6547f9aa5d1fae3528ebfb","max":498818054,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eff5291aa6714dcb940b561d72b361ee","tabbable":null,"tooltip":null,"value":498818054}},"a9f8196ea5af45a18d8564411de39f46":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae105f836ad74fbe9294f22d6884670c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_94f8d9af130941449fb43eed21934e31","IPY_MODEL_02780c5ff73e472c88e127984e2357fb","IPY_MODEL_c13bf52cb502473381ebb922aefb2c06"],"layout":"IPY_MODEL_fe398e44014c4467bc9cdb059e5f35e0","tabbable":null,"tooltip":null}},"b08f56664fba493c80962b44a7df09ff":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1b9f168fbe745269135c979ae4c6b8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b78ed570c4474257a677d76e22c834b5":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8abcd33c5e348249b7d237949e9ba06":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"b9a82bd626dc4710b88c8ae41062c299":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c13bf52cb502473381ebb922aefb2c06":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_b9a82bd626dc4710b88c8ae41062c299","placeholder":"​","style":"IPY_MODEL_9d4e0782654946768a414ad6251b01aa","tabbable":null,"tooltip":null,"value":" 456k/456k [00:00&lt;00:00, 1.14MB/s]"}},"c18dea3328aa41709d757712b1a4a9ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_54d53354e60b4607af55ab97a67a9e9a","placeholder":"​","style":"IPY_MODEL_42ab82dc67f649edbddd2e8913e5f74a","tabbable":null,"tooltip":null,"value":"config.json: 100%"}},"c3b5fc2096a248f6aba5adfc4b311696":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_a18d732a0c2c4e979116cbb75e64055b","placeholder":"​","style":"IPY_MODEL_986abfab1b3049dfb41759fea2d09462","tabbable":null,"tooltip":null,"value":" 481/481 [00:00&lt;00:00, 48.4kB/s]"}},"c5678c1987ae403b84bc41cd8a92d5e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_ce15b5bd95f74c67ac0234b648e89645","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3f40f2bfef8640a18909e77960968817","tabbable":null,"tooltip":null,"value":481}},"ce15b5bd95f74c67ac0234b648e89645":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5af7e751cfe4f439c5168b99a3f303c":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dfb1a714a8504102a9186e36cf2a33c5":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_b78ed570c4474257a677d76e22c834b5","placeholder":"​","style":"IPY_MODEL_847f82addeb54586a493704de6afa8f1","tabbable":null,"tooltip":null,"value":"tokenizer_config.json: 100%"}},"e14c86265ec2455ab90cbcbfd1c7b1d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"e247d23083cd4f03b369b4f8ecc17847":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e38fbd50853a4594add57a67c7e74f75":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_3a6c4520a5734c2e8438edd77d81c516","placeholder":"​","style":"IPY_MODEL_0913c3e7286f4b508c80510346313743","tabbable":null,"tooltip":null,"value":" 1.36M/1.36M [00:00&lt;00:00, 1.73MB/s]"}},"e680fc104e564811a9326636de1add92":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e84caa896d24468589215e139d639568":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_722ffb73aac64935ba9b3e818563319b","placeholder":"​","style":"IPY_MODEL_ea0d49948c3c4cc58c85c478e05e13e6","tabbable":null,"tooltip":null,"value":"model.safetensors: 100%"}},"ea0d49948c3c4cc58c85c478e05e13e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"eff5291aa6714dcb940b561d72b361ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f57e08390fea4ae693f9ecc259d3d299":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dfb1a714a8504102a9186e36cf2a33c5","IPY_MODEL_35873a083d054258bd7499731044af57","IPY_MODEL_2f7393ef14f24eb8923a2ce478403e89"],"layout":"IPY_MODEL_19783d1739ef4c4c9099e7716d064aff","tabbable":null,"tooltip":null}},"f684dc9efc6547f9aa5d1fae3528ebfb":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7dcd8c00b1c433686b7f4b5d5f46243":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa0512ec1149487691065beaeaf7b6d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"fa75eeaf9ef14ae98f169d9d52dc2445":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbfae1778f154d058d0ed632f581852d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_fa75eeaf9ef14ae98f169d9d52dc2445","placeholder":"​","style":"IPY_MODEL_0c6df18a309b4f3792f79d130c2c4f1f","tabbable":null,"tooltip":null,"value":"vocab.json: 100%"}},"fe398e44014c4467bc9cdb059e5f35e0":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sarcasm Detection using RoBERTa and Emoji Embeddings\n\nIn this notebook, we will build a sarcasm detector that leverages both textual data processed by BERT and emoji embeddings processed through a custom emoji pathway. The model combines these modalities to predict whether a given text is sarcastic or not.","metadata":{"papermill":{"duration":0.006006,"end_time":"2025-01-18T15:17:00.118463","exception":false,"start_time":"2025-01-18T15:17:00.112457","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# DATASET analysis\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# File paths\ntrain_path = '/kaggle/input/ghosh-sarcasm-detection-dataset/clean_data.txt'\ntest_path = '/kaggle/input/ghosh-sarcasm-detection-dataset/test.txt'\n\n# Function to load data\ndef load_data(file_path):\n    texts = []\n    labels = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue  # Skip empty lines\n            try:\n                text, label = line.rsplit(' ', 1)\n                texts.append(text.strip())\n                labels.append(int(label.strip()))\n            except ValueError:\n                continue  # Skip lines that don't have at least one space\n    return pd.DataFrame({'text': texts, 'label': labels})\n\n# Load datasets\ntrain_df = load_data(train_path)\ntest_df = load_data(test_path)\n\n# Combine datasets for analysis (if needed)\n# full_df = pd.concat([train_df, test_df], ignore_index=True)\n\n# Analyze class distribution in training set\ntrain_counts = train_df['label'].value_counts().sort_index()\nprint(\"Training set class distribution:\")\nprint(train_counts)\n\n# Plot the class distribution\nsns.countplot(x='label', data=train_df)\nplt.title('Training Set Class Distribution')\nplt.xlabel('Class Label')\nplt.ylabel('Count')\nplt.show()\n\n# Analyze class distribution in test set\ntest_counts = test_df['label'].value_counts().sort_index()\nprint(\"\\nTest set class distribution:\")\nprint(test_counts)\n\n# Plot the class distribution\nsns.countplot(x='label', data=test_df)\nplt.title('Test Set Class Distribution')\nplt.xlabel('Class Label')\nplt.ylabel('Count')\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-01-18T15:17:00.129389Z","iopub.status.busy":"2025-01-18T15:17:00.129120Z","iopub.status.idle":"2025-01-18T15:17:02.408431Z","shell.execute_reply":"2025-01-18T15:17:02.407462Z"},"papermill":{"duration":2.286211,"end_time":"2025-01-18T15:17:02.409972","exception":false,"start_time":"2025-01-18T15:17:00.123761","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 0. Augumentation ","metadata":{}},{"cell_type":"code","source":"# ---------------------------\n# Ensure NLTK WordNet is ready (downloaded and unzipped)\n# ---------------------------\nimport os\nimport zipfile\nimport nltk\n\n# Set the NLTK data directory (Kaggle typically uses '/root/nltk_data')\nnltk_data_dir = '/root/nltk_data'\nif not os.path.exists(nltk_data_dir):\n    os.makedirs(nltk_data_dir)\n\n# Add the directory to nltk data path if not already there\nif nltk_data_dir not in nltk.data.path:\n    nltk.data.path.append(nltk_data_dir)\n\n# First, download wordnet (this will not re-download if it's up-to-date)\nnltk.download('wordnet', download_dir=nltk_data_dir, quiet=True)\nnltk.download('omw-1.4', download_dir=nltk_data_dir, quiet=True)\n\n# Check if WordNet corpus is unzipped\nwordnet_dir = os.path.join(nltk_data_dir, 'corpora', 'wordnet')\nif not os.path.isdir(wordnet_dir):\n    # Look for the zipped file\n    wordnet_zip = os.path.join(nltk_data_dir, 'corpora', 'wordnet.zip')\n    if os.path.exists(wordnet_zip):\n        print(\"Unzipping wordnet.zip ...\")\n        with zipfile.ZipFile(wordnet_zip, 'r') as zip_ref:\n            zip_ref.extractall(os.path.join(nltk_data_dir, 'corpora'))\n        print(\"Unzipping complete.\")\n    else:\n        print(\"WordNet data not found. Please check your NLTK installation.\")\n\n# ---------------------------\n# Now define your augmentation functions (EDA)\n# ---------------------------\nimport random\nfrom nltk.corpus import wordnet\n\ndef get_synonyms(word):\n    \"\"\"Get a list of synonyms for a given word from WordNet.\"\"\"\n    synonyms = set()\n    for syn in wordnet.synsets(word):\n        for lemma in syn.lemmas():\n            synonym = lemma.name().replace('_', ' ').lower()\n            if synonym != word.lower():\n                synonyms.add(synonym)\n    return list(synonyms)\n\ndef synonym_replacement(words, n):\n    \"\"\"Replace n words in the sentence with one of their synonyms.\"\"\"\n    new_words = words.copy()\n    random_word_list = list(set([word for word in words if get_synonyms(word)]))\n    random.shuffle(random_word_list)\n    num_replaced = 0\n    for word in random_word_list:\n        synonyms = get_synonyms(word)\n        if synonyms:\n            synonym = random.choice(synonyms)\n            new_words = [synonym if w == word else w for w in new_words]\n            num_replaced += 1\n        if num_replaced >= n:\n            break\n    return new_words\n\ndef random_insertion(words, n):\n    \"\"\"Randomly insert n synonyms into the sentence.\"\"\"\n    new_words = words.copy()\n    for _ in range(n):\n        add_word(new_words)\n    return new_words\n\ndef add_word(new_words):\n    \"\"\"Helper function to insert a random synonym into the sentence.\"\"\"\n    synonyms = []\n    counter = 0\n    while len(synonyms) < 1 and counter < 10:\n        random_word = new_words[random.randint(0, len(new_words)-1)]\n        synonyms = get_synonyms(random_word)\n        counter += 1\n    if synonyms:\n        random_synonym = random.choice(synonyms)\n        random_idx = random.randint(0, len(new_words))\n        new_words.insert(random_idx, random_synonym)\n\ndef random_swap(words, n):\n    \"\"\"Randomly swap two words in the sentence n times.\"\"\"\n    new_words = words.copy()\n    for _ in range(n):\n        new_words = swap_word(new_words)\n    return new_words\n\ndef swap_word(words):\n    \"\"\"Helper function to swap two random words in the list.\"\"\"\n    new_words = words.copy()\n    idx1 = random.randint(0, len(new_words)-1)\n    idx2 = idx1\n    counter = 0\n    while idx2 == idx1 and counter < 3:\n        idx2 = random.randint(0, len(new_words)-1)\n        counter += 1\n    new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n    return new_words\n\ndef random_deletion(words, p):\n    \"\"\"Randomly delete words from the sentence with probability p.\"\"\"\n    if len(words) == 1:\n        return words  # Return if only one word.\n    new_words = []\n    for word in words:\n        if random.uniform(0, 1) > p:\n            new_words.append(word)\n    if not new_words:\n        return [random.choice(words)]\n    return new_words\n\ndef eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=4):\n    \"\"\"\n    Apply Easy Data Augmentation (EDA) on the input sentence.\n    Returns a list of augmented sentences.\n    \"\"\"\n    words = sentence.split()\n    num_words = len(words)\n    augmented_sentences = []\n    n_sr = max(1, int(alpha_sr * num_words))\n    n_ri = max(1, int(alpha_ri * num_words))\n    n_rs = max(1, int(alpha_rs * num_words))\n    \n    # 1. Synonym Replacement\n    a_words = synonym_replacement(words, n_sr)\n    augmented_sentences.append(\" \".join(a_words))\n    \n    # 2. Random Insertion\n    a_words = random_insertion(words, n_ri)\n    augmented_sentences.append(\" \".join(a_words))\n    \n    # 3. Random Swap\n    a_words = random_swap(words, n_rs)\n    augmented_sentences.append(\" \".join(a_words))\n    \n    # 4. Random Deletion\n    a_words = random_deletion(words, p_rd)\n    augmented_sentences.append(\" \".join(a_words))\n    \n    while len(augmented_sentences) < num_aug:\n        augmented_sentences.append(random.choice(augmented_sentences))\n    \n    return augmented_sentences[:num_aug]\n\n# ---------------------------\n# Functions to load and augment training data\n# ---------------------------\ndef load_training_data(file_path):\n    \"\"\"\n    Load training data from a file where each line is 'text label'.\n    \"\"\"\n    texts = []\n    labels = []\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            try:\n                text, label = line.rsplit(' ', 1)\n                texts.append(text.strip())\n                labels.append(int(label.strip()))\n            except Exception as e:\n                print(f\"Error processing line: {line}\\n{e}\")\n                continue\n    return texts, labels\n\ndef augment_training_data(input_path, output_path, num_aug=2):\n    \"\"\"\n    Augment the training data and save to a new file.\n    Each original sentence is retained along with 'num_aug' augmented variants.\n    \"\"\"\n    texts, labels = load_training_data(input_path)\n    augmented_texts = []\n    augmented_labels = []\n    \n    for text, label in zip(texts, labels):\n        # Add the original sentence\n        augmented_texts.append(text)\n        augmented_labels.append(label)\n        # Generate augmented sentences\n        aug_texts = eda(text, num_aug=num_aug)\n        augmented_texts.extend(aug_texts)\n        augmented_labels.extend([label] * len(aug_texts))\n    \n    with open(output_path, 'w', encoding='utf-8') as f:\n        for text, label in zip(augmented_texts, augmented_labels):\n            f.write(f\"{text} {label}\\n\")\n    print(f\"Augmented training data saved to {output_path}\")\n\n# ---------------------------\n# Run the augmentation\n# ---------------------------\ntrain_input_path = '/kaggle/input/ghosh-sarcasm-detection-dataset/clean_data.txt'\ntrain_augmented_path = '/kaggle/working/train_aug.txt'\n\n# For example, generating 2 augmented sentences per original sentence.\naugment_training_data(train_input_path, train_augmented_path, num_aug=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T03:00:54.339161Z","iopub.execute_input":"2025-02-12T03:00:54.339490Z","iopub.status.idle":"2025-02-12T03:01:22.056700Z","shell.execute_reply.started":"2025-02-12T03:00:54.339452Z","shell.execute_reply":"2025-02-12T03:01:22.055821Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1. Import Necessary Libraries","metadata":{"papermill":{"duration":0.005992,"end_time":"2025-01-18T15:17:02.423445","exception":false,"start_time":"2025-01-18T15:17:02.417453","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertModel\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nimport os\nimport numpy as np\nfrom typing import List\nimport gensim.models as gsm\nfrom pathlib import Path\nimport argparse\nimport time\nimport matplotlib.pyplot as plt \nimport pickle\n\nfrom transformers import RobertaModel, RobertaTokenizer","metadata":{"execution":{"iopub.status.busy":"2025-02-12T03:02:10.992890Z","iopub.execute_input":"2025-02-12T03:02:10.993178Z","iopub.status.idle":"2025-02-12T03:02:50.646171Z","shell.execute_reply.started":"2025-02-12T03:02:10.993156Z","shell.execute_reply":"2025-02-12T03:02:50.645497Z"},"papermill":{"duration":15.973398,"end_time":"2025-01-18T15:17:18.402328","exception":false,"start_time":"2025-01-18T15:17:02.428930","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Define helper functions","metadata":{"papermill":{"duration":0.008818,"end_time":"2025-01-18T15:17:18.421863","exception":false,"start_time":"2025-01-18T15:17:18.413045","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#utils_emoji.py\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer\nfrom transformers import RobertaModel, RobertaTokenizer\nimport pandas as pd\nimport numpy as np\n\nclass SarcasmDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        \n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long),\n            'raw_text': text\n        }\n\ndef prepare_bert_data(train_path, val_path, test_path, tokenizer, batch_size=16):\n    # Function to parse data files and create data loaders\n\n    def load_data(file_path):\n        texts, labels = [], []\n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue  # Skip empty lines\n                try:\n                    text, label = line.rsplit(' ', 1)\n                    texts.append(text.strip())\n                    labels.append(int(label.strip()))\n                except ValueError:\n                    continue  # Skip lines that don't have at least one space\n        return texts, labels\n\n    # Load training data\n    train_texts, train_labels = load_data(train_path)\n\n    # Load validation data\n    val_texts, val_labels = load_data(val_path)\n\n    # Load test data\n    test_texts, test_labels = load_data(test_path)\n\n    # Create datasets\n    train_dataset = SarcasmDataset(train_texts, train_labels, tokenizer)\n    val_dataset = SarcasmDataset(val_texts, val_labels, tokenizer)\n    test_dataset = SarcasmDataset(test_texts, test_labels, tokenizer)\n\n    \n    # Define collate function\n    def collate_fn(batch):\n        return {\n            'input_ids': torch.stack([item['input_ids'] for item in batch]),\n            'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n            'labels': torch.stack([item['labels'] for item in batch]),\n            'raw_text': [item['raw_text'] for item in batch]\n        }\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    \n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    \n    return train_loader, val_loader, test_loader, tokenizer\n\n# if __name__ == \"__main__\":\n#     train_loader, val_loader, test_loader, tokenizer = prepare_bert_data(\n#         'data/riloff/train.txt',\n#         'data/riloff/test.txt',\n#         batch_size=16\n#     )\n    \n#     batch = next(iter(train_loader))\n#     print(\"Input shape:\", batch['input_ids'].shape)\n#     print(\"Attention mask shape:\", batch['attention_mask'].shape)\n#     print(\"Labels shape:\", batch['labels'].shape)\n#     print(\"Raw text length:\", len(batch['raw_text']))","metadata":{"execution":{"iopub.status.busy":"2025-02-12T03:02:50.647179Z","iopub.execute_input":"2025-02-12T03:02:50.647762Z","iopub.status.idle":"2025-02-12T03:02:50.663571Z","shell.execute_reply.started":"2025-02-12T03:02:50.647737Z","shell.execute_reply":"2025-02-12T03:02:50.662946Z"},"papermill":{"duration":0.025997,"end_time":"2025-01-18T15:17:18.453295","exception":false,"start_time":"2025-01-18T15:17:18.427298","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#split.py\n\n\nimport random\nimport shutil\n\n# File paths\ninput_path = '/kaggle/working/train_aug.txt'  # Input data to split into train and val       \ntest_input_path = '/kaggle/input/ghosh-sarcasm-detection-dataset/test.txt'   # Test data is already defined\ntrain_output_path = '/kaggle/working/train.txt'   # Output path for training data\nval_output_path = '/kaggle/working/val.txt'       # Output path for validation data\ntest_output_path = '/kaggle/working/test.txt'     # Output path for testing data\n\n# Load data from clean_data.txt\ndata = []\nwith open(input_path, 'r', encoding='utf-8') as file:\n    for line in file:\n        line = line.strip()\n        if not line:\n            continue  # Skip empty lines\n        # Split line into text and label\n        # Since the label is at the end, we can split from the right\n        try:\n            text, label = line.rsplit(' ', 1)\n            data.append((text.strip(), label.strip()))\n        except ValueError:\n            continue  # Skip lines that don't have at least one space\n\n# Shuffle data\nrandom.shuffle(data)\n\n# Split data into train and validation sets\nsplit_ratio = 0.9  # Use 90% of data for training, 10% for validation\nsplit_index = int(split_ratio * len(data))\ntrain_data = data[:split_index]\nval_data = data[split_index:]\n\n# Save training data to file\nwith open(train_output_path, 'w', encoding='utf-8') as train_file:\n    for text, label in train_data:\n        train_file.write(f\"{text} {label}\\n\")  # Write as 'text label'\n\n# Save validation data to file\nwith open(val_output_path, 'w', encoding='utf-8') as val_file:\n    for text, label in val_data:\n        val_file.write(f\"{text} {label}\\n\")\n\n# Copy test data to /kaggle/working/test.txt (if needed)\ntest_source_path = '/kaggle/input/ghosh-sarcasm-detection-dataset/test.txt'\n# If the test data is not already in the correct format, process it similarly\nwith open(test_source_path, 'r', encoding='utf-8') as test_input_file, \\\n     open(test_output_path, 'w', encoding='utf-8') as test_output_file:\n    for line in test_input_file:\n        line = line.strip()\n        if not line:\n            continue  # Skip empty lines\n        # Assuming test data is in the same format as clean_data.txt\n        try:\n            text, label = line.rsplit(' ', 1)\n            test_output_file.write(f\"{text} {label}\\n\")\n        except ValueError:\n            continue  # Skip lines that don't have at least one space\n\nprint(\"Data has been split and saved to 'train.txt', 'val.txt', and 'test.txt' in the /kaggle/working directory.\")","metadata":{"execution":{"iopub.status.busy":"2025-02-12T03:02:50.665309Z","iopub.execute_input":"2025-02-12T03:02:50.665589Z","iopub.status.idle":"2025-02-12T03:02:51.074799Z","shell.execute_reply.started":"2025-02-12T03:02:50.665559Z","shell.execute_reply":"2025-02-12T03:02:51.074074Z"},"papermill":{"duration":0.131487,"end_time":"2025-01-18T15:17:18.611297","exception":false,"start_time":"2025-01-18T15:17:18.479810","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check training data\nprint(\"First few lines of train.txt:\")\nwith open('/kaggle/working/train.txt', 'r', encoding='utf-8') as f:\n    for _ in range(5):\n        print(next(f).strip())\n\n# Check validation data\nprint(\"\\nFirst few lines of val.txt:\")\nwith open('/kaggle/working/val.txt', 'r', encoding='utf-8') as f:\n    for _ in range(5):\n        print(next(f).strip())\n\n# Check test data\nprint(\"\\nFirst few lines of test.txt:\")\nwith open('/kaggle/working/test.txt', 'r', encoding='utf-8') as f:\n    for _ in range(5):\n        print(next(f).strip())","metadata":{"execution":{"iopub.status.busy":"2025-02-12T03:02:51.075887Z","iopub.execute_input":"2025-02-12T03:02:51.076201Z","iopub.status.idle":"2025-02-12T03:02:51.085784Z","shell.execute_reply.started":"2025-02-12T03:02:51.076171Z","shell.execute_reply":"2025-02-12T03:02:51.085108Z"},"papermill":{"duration":0.018271,"end_time":"2025-01-18T15:17:18.640257","exception":false,"start_time":"2025-01-18T15:17:18.621986","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Define the EmojiEncoder Class\nThe EmojiEncoder extracts emojis from text and converts them into embeddings using a pre-trained emoji2vec model.","metadata":{"papermill":{"duration":0.005638,"end_time":"2025-01-18T15:17:18.652031","exception":false,"start_time":"2025-01-18T15:17:18.646393","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class EmojiEncoder(nn.Module):\n    \"\"\"Encodes emoji sequences into fixed-dimensional embeddings.\"\"\"\n    \n    def __init__(self, emoji_dim: int = 300, model_path: str = '/kaggle/input/code_modules/tensorflow2/default/1/emoji2vec.bin'):\n        \"\"\"\n        Args:\n            emoji_dim: Output dimension of emoji embeddings\n            model_path: Path to pre-trained emoji2vec binary file\n        \"\"\"\n        super().__init__()\n        \n        if not Path(model_path).exists():\n            raise FileNotFoundError(f\"emoji2vec model not found at {model_path}\")\n            \n        self.emoji2vec = gsm.KeyedVectors.load_word2vec_format(model_path, binary=True)\n        self.projection = nn.Linear(self.emoji2vec.vector_size, emoji_dim)\n        \n    def forward(self, texts: List[str]) -> torch.Tensor:\n        \"\"\"\n        Convert text sequences containing emojis to fixed-length embeddings.\n        \n        Args:\n            texts: List of strings potentially containing emojis\n            \n        Returns:\n            Tensor of shape (batch_size, seq_len, emoji_dim) containing emoji embeddings\n        \"\"\"\n        batch_size = len(texts)\n        device = next(self.projection.parameters()).device\n        \n        # Pre-allocate output tensor - adding sequence length dimension\n        emoji_embeddings = torch.zeros(batch_size, 128, self.emoji2vec.vector_size)\n        \n        for i, text in enumerate(texts):\n            # Extract emojis present in vocabulary\n            emojis = [c for c in text if c in self.emoji2vec.key_to_index]\n            \n            if emojis:\n                # Get embeddings for all emojis in text\n                emoji_vecs = [self.emoji2vec[emoji] for emoji in emojis]\n                # Stack emoji vectors along sequence dimension\n                emoji_seq = torch.tensor(np.stack(emoji_vecs))\n                # Pad or truncate to fixed sequence length\n                if len(emoji_seq) > 128:\n                    emoji_embeddings[i] = emoji_seq[:128]\n                else:\n                    emoji_embeddings[i, :len(emoji_seq)] = emoji_seq\n                \n        # Move to same device as model and project each embedding in the sequence\n        emoji_embeddings = emoji_embeddings.to(device)\n        emoji_embeddings = self.projection(emoji_embeddings)  # Shape: [batch_size, seq_len, emoji_dim]\n        return emoji_embeddings","metadata":{"execution":{"iopub.status.busy":"2025-02-12T03:02:51.086521Z","iopub.execute_input":"2025-02-12T03:02:51.086812Z","iopub.status.idle":"2025-02-12T03:02:51.100077Z","shell.execute_reply.started":"2025-02-12T03:02:51.086791Z","shell.execute_reply":"2025-02-12T03:02:51.099442Z"},"papermill":{"duration":0.018448,"end_time":"2025-01-18T15:17:18.676578","exception":false,"start_time":"2025-01-18T15:17:18.658130","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4.Define the Attention Mechanism\nThe Attention class implements an attention mechanism to focus on important parts of the sequence.","metadata":{"papermill":{"duration":0.005734,"end_time":"2025-01-18T15:17:18.690006","exception":false,"start_time":"2025-01-18T15:17:18.684272","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, lstm_hidden_size):\n        super(Attention, self).__init__()\n        self.attention = nn.Linear(lstm_hidden_size * 2, 1)\n\n    def forward(self, lstm_out):\n        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n        return context_vector","metadata":{"execution":{"iopub.status.busy":"2025-02-12T03:02:51.100817Z","iopub.execute_input":"2025-02-12T03:02:51.101035Z","iopub.status.idle":"2025-02-12T03:02:51.120582Z","shell.execute_reply.started":"2025-02-12T03:02:51.101008Z","shell.execute_reply":"2025-02-12T03:02:51.119769Z"},"papermill":{"duration":0.011713,"end_time":"2025-01-18T15:17:18.707557","exception":false,"start_time":"2025-01-18T15:17:18.695844","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5. Define the SarcasmDetector Model\nThe SarcasmDetector combines BERT embeddings and emoji embeddings, processes them through separate pathways, and then fuses the features for classification.","metadata":{"papermill":{"duration":0.005688,"end_time":"2025-01-18T15:17:18.719805","exception":false,"start_time":"2025-01-18T15:17:18.714117","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class SarcasmDetector(nn.Module):\n    def __init__(self, dropout_rate=0.5, freeze_bert=False):\n        super(SarcasmDetector, self).__init__()\n        \n        # Replace BERT with RoBERTa\n        self.bert = RobertaModel.from_pretrained('roberta-base')\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        self.bert_dim = self.bert.config.hidden_size  # Typically 768 for roberta-base\n        \n        # Add layer normalization\n        self.bert_norm = nn.LayerNorm(self.bert_dim)\n        self.emoji_norm = nn.LayerNorm(300)  # for emoji embeddings\n        \n        # Emoji components\n        self.emoji_encoder = EmojiEncoder()\n        \n        # Channels and sizes\n        self.cnn_out_channels = 256\n        self.lstm_hidden_size = 256  # doubled\n        self.dense_hidden_size = 256  # doubled\n        \n        # BERT pathway layers\n        self.bert_conv = nn.Conv1d(\n            in_channels=self.bert_dim,\n            out_channels=self.cnn_out_channels,\n            kernel_size=3,\n            padding=1\n        )\n        \n        self.bert_lstm = nn.LSTM(\n            input_size=self.cnn_out_channels,\n            hidden_size=self.lstm_hidden_size,\n            num_layers=2,\n            bidirectional=True,\n            batch_first=True,\n            dropout=dropout_rate\n        )\n        \n        self.bert_attention = Attention(self.lstm_hidden_size)\n        \n        # Emoji pathway layers\n        self.emoji_conv = nn.Conv1d(\n            in_channels=300,  # Original emoji dimension\n            out_channels=self.cnn_out_channels,\n            kernel_size=3,\n            padding=1\n        )\n        \n        self.emoji_lstm = nn.LSTM(\n            input_size=self.cnn_out_channels,\n            hidden_size=self.lstm_hidden_size,\n            num_layers=2,\n            bidirectional=True,\n            batch_first=True,\n            dropout=dropout_rate\n        )\n        \n        self.emoji_attention = Attention(self.lstm_hidden_size)\n        \n        # Fusion and classification layers\n        combined_features_size = (self.lstm_hidden_size * 4)  # 2 * (lstm_hidden_size * 2)\n        self.fusion = nn.Linear(combined_features_size, self.dense_hidden_size)\n        \n        self.dense1 = nn.Linear(self.dense_hidden_size, self.dense_hidden_size)\n        self.dense2 = nn.Linear(self.dense_hidden_size, 2)\n        \n        self.dropout = nn.Dropout(dropout_rate)\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)\n\n        # Add batch normalization\n        self.bert_bn = nn.BatchNorm1d(self.cnn_out_channels)\n        self.emoji_bn = nn.BatchNorm1d(self.cnn_out_channels)\n        \n        # Add residual connections\n        self.bert_residual = nn.Linear(self.bert_dim, self.lstm_hidden_size * 2)  # Match LSTM bidirectional output\n        self.emoji_residual = nn.Linear(300, self.lstm_hidden_size * 2)  # Match LSTM bidirectional output\n\n    def process_bert_features(self, embeddings):\n        cnn_in = embeddings.permute(0, 2, 1)\n        cnn_out = self.relu(self.bert_conv(cnn_in))\n        lstm_in = cnn_out.permute(0, 2, 1)\n        \n        lstm_out, _ = self.bert_lstm(lstm_in)\n        features = self.bert_attention(lstm_out)\n        return features\n\n    def process_emoji_features(self, embeddings):\n        cnn_in = embeddings.permute(0, 2, 1)\n        cnn_out = self.relu(self.emoji_conv(cnn_in))\n        lstm_in = cnn_out.permute(0, 2, 1)\n        \n        lstm_out, _ = self.emoji_lstm(lstm_in)\n        features = self.emoji_attention(lstm_out)\n        return features\n\n    def forward(self, input_ids, attention_mask, raw_texts):\n        # Process BERT with residual connection\n        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        bert_embeddings = self.bert_norm(bert_output.last_hidden_state)\n        bert_residual = self.bert_residual(bert_embeddings.mean(dim=1))  # [batch, lstm_hidden_size * 2]\n        \n        # Process emoji with residual connection\n        emoji_embeddings = self.emoji_encoder(raw_texts)\n        emoji_embeddings = self.emoji_norm(emoji_embeddings)\n        emoji_residual = self.emoji_residual(emoji_embeddings.mean(dim=1))  # [batch, lstm_hidden_size * 2]\n        \n        # Main pathways with proper dimensions\n        text_features = self.process_bert_features(bert_embeddings)  # [batch, lstm_hidden_size * 2]\n        emoji_features = self.process_emoji_features(emoji_embeddings)  # [batch, lstm_hidden_size * 2]\n        \n        # Now dimensions match for residual connections\n        text_features = text_features + bert_residual  # Both are [batch, lstm_hidden_size * 2]\n        emoji_features = emoji_features + emoji_residual  # Both are [batch, lstm_hidden_size * 2]\n        \n        # Combine features\n        combined_features = torch.cat([text_features, emoji_features], dim=1)\n        \n        # Continue through dense layers\n        x = self.fusion(combined_features)\n        x = self.dense1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        logits = self.dense2(x)\n        predictions = self.softmax(logits)\n        \n        return predictions","metadata":{"execution":{"iopub.status.busy":"2025-02-12T03:03:08.819945Z","iopub.execute_input":"2025-02-12T03:03:08.820256Z","iopub.status.idle":"2025-02-12T03:03:08.837127Z","shell.execute_reply.started":"2025-02-12T03:03:08.820214Z","shell.execute_reply":"2025-02-12T03:03:08.836192Z"},"papermill":{"duration":0.018874,"end_time":"2025-01-18T15:17:18.744620","exception":false,"start_time":"2025-01-18T15:17:18.725746","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 7. Define Training and Evaluation Functions\nThe train_epoch function handles training for one epoch, and the evaluate function evaluates the model on the validation or test set.","metadata":{"papermill":{"duration":0.005655,"end_time":"2025-01-18T15:17:18.756299","exception":false,"start_time":"2025-01-18T15:17:18.750644","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def train_epoch(model, train_loader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    start_time = time.time()\n    total_steps = len(train_loader)\n    \n    for batch_idx, batch in enumerate(train_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        raw_texts = batch['raw_text']\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, raw_texts)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n        optimizer.step()\n        \n        total_loss += loss.item()\n        \n        # Print progress every 10 batches\n        if (batch_idx + 1) % 10 == 0:\n            print(f'  Step [{batch_idx + 1}/{total_steps}], Loss: {loss.item():.4f}')\n    \n    epoch_time = time.time() - start_time\n    return total_loss / len(train_loader), epoch_time\ndef evaluate(model, test_loader, criterion, device, return_predictions=False):\n    model.eval()\n    total_loss = 0\n    correct = 0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch_idx, batch in enumerate(test_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            raw_texts = batch['raw_text']  # If needed\n\n            outputs = model(input_ids, attention_mask, raw_texts)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n\n            preds = outputs.argmax(dim=1)\n            correct += (preds == labels).sum().item()\n\n            if return_predictions:\n                batch_preds = preds.cpu().numpy()\n                batch_labels = labels.cpu().numpy()\n                all_preds.extend(batch_preds)\n                all_labels.extend(batch_labels)\n\n            # Remove or comment out the batch predictions print statements\n            # if batch_idx < 3:\n            #     print(f\"\\nBatch {batch_idx}:\")\n            #     for i in range(min(5, len(batch_preds))):\n            #         print(f\"Pred: {batch_preds[i]}, True: {batch_labels[i]}\")\n\n    avg_loss = total_loss / len(test_loader)\n    accuracy = correct / len(test_loader.dataset)\n\n    if return_predictions:\n        all_preds = np.array(all_preds)\n        all_labels = np.array(all_labels)\n        return avg_loss, accuracy, all_preds, all_labels\n    else:\n        return avg_loss, accuracy","metadata":{"execution":{"iopub.status.busy":"2025-02-12T03:03:12.308428Z","iopub.execute_input":"2025-02-12T03:03:12.308714Z","iopub.status.idle":"2025-02-12T03:03:12.317675Z","shell.execute_reply.started":"2025-02-12T03:03:12.308693Z","shell.execute_reply":"2025-02-12T03:03:12.316698Z"},"papermill":{"duration":0.015509,"end_time":"2025-01-18T15:17:18.777601","exception":false,"start_time":"2025-01-18T15:17:18.762092","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 8.Load Dataset and Preprocess Data\nPrepare the data loaders for training and testing.","metadata":{"papermill":{"duration":0.005573,"end_time":"2025-01-18T15:17:18.789020","exception":false,"start_time":"2025-01-18T15:17:18.783447","status":"completed"},"tags":[]}},{"cell_type":"code","source":"'''# Parse arguments (if running as a script, you can modify or remove argparse when using in a notebook)\nparser = argparse.ArgumentParser(description='Train and evaluate Sarcasm Detector')\nparser.add_argument('--dataset', type=str, help='Name of the dataset', default='Mishra')\n\nargs, unknown = parser.parse_known_args()\ntrain_path = f'data/{args.dataset}/train.txt'\ntest_path = f'data/{args.dataset}/test.txt' '''\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)\n\n# Paths to your data files\ntrain_path = '/kaggle/working/train.txt'\nval_path = '/kaggle/working/val.txt'\ntest_path = '/kaggle/working/test.txt'\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)\n\nfrom transformers import RobertaTokenizer\n\n# Initialize the RoBERTa tokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n# Prepare data loaders\ntrain_loader, val_loader, test_loader, tokenizer = prepare_bert_data(\n    train_path=train_path,\n    val_path=val_path,\n    test_path=test_path,\n    tokenizer=tokenizer,        # Pass the RoBERTa tokenizer\n    batch_size=32\n)","metadata":{"execution":{"iopub.status.busy":"2025-02-12T03:03:14.348175Z","iopub.execute_input":"2025-02-12T03:03:14.348521Z","iopub.status.idle":"2025-02-12T03:03:16.728161Z","shell.execute_reply.started":"2025-02-12T03:03:14.348494Z","shell.execute_reply":"2025-02-12T03:03:16.727461Z"},"papermill":{"duration":4.753014,"end_time":"2025-01-18T15:17:23.547757","exception":false,"start_time":"2025-01-18T15:17:18.794743","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 9. Initialize the Model, Optimizer, and Criterion\nSet up the model, define the optimizer with different learning rates for different parts of the model, and specify the loss function.","metadata":{"papermill":{"duration":0.007111,"end_time":"2025-01-18T15:17:23.562327","exception":false,"start_time":"2025-01-18T15:17:23.555216","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model = SarcasmDetector(dropout_rate=0.5, freeze_bert=False).to(device)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total number of parameters: {total_params}\")\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Trainable parameters: {trainable_params}\")\n\n# Separate RoBERTa and task-specific parameters for different learning rates\noptimizer = torch.optim.AdamW([\n    {'params': model.bert.parameters(), 'lr': 2e-5},\n    {'params': model.emoji_encoder.parameters(), 'lr': 1e-4},\n    {'params': [p for n, p in model.named_parameters() \n                if not n.startswith('bert.') and not n.startswith('emoji_encoder.')], \n     'lr': 1e-3}\n])\n\n# Define the loss function\n\n# Focal Loss instead of CrossEntropy\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2):\n        super().__init__()\n        self.gamma = gamma\n            \n    def forward(self, input, target):\n        ce_loss = nn.functional.cross_entropy(input, target, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n        return focal_loss\n            \ncriterion = FocalLoss(gamma=2)\n\n# Define the learning rate scheduler\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer, \n    T_max=25,  # number of epochs\n    eta_min=1e-6\n)\n\n# Gradient clipping\nmax_grad_norm = 1.0","metadata":{"execution":{"iopub.status.busy":"2025-02-12T03:03:16.729367Z","iopub.execute_input":"2025-02-12T03:03:16.729666Z","iopub.status.idle":"2025-02-12T03:03:20.304135Z","shell.execute_reply.started":"2025-02-12T03:03:16.729639Z","shell.execute_reply":"2025-02-12T03:03:20.303347Z"},"papermill":{"duration":22.087085,"end_time":"2025-01-18T15:17:45.656348","exception":false,"start_time":"2025-01-18T15:17:23.569263","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 10. Train the Model\nTrain the model over multiple epochs, implement early stopping, and save the best model.","metadata":{"papermill":{"duration":0.00658,"end_time":"2025-01-18T15:17:45.670329","exception":false,"start_time":"2025-01-18T15:17:45.663749","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Initialize metric storage\ntrain_losses = []\ntrain_accuracies = []\nval_losses = []\nval_accuracies = []\nepochs_list = []  # Renamed to avoid conflict with built-in function 'epochs'\n\n# Initialize total training time\ntotal_train_time = 0  # Initialize to zero before the training loop\n\n# Define the maximum training time in seconds (8 hours)\nmax_training_time = 8 * 60 * 60  # 8 hours in seconds\n\n# Initialize early stopping variables\nbest_loss = float('inf')\npatience = 1\npatience_counter = 0\nmodel_path = '/kaggle/working/sarcasm_detector_model.pth'\n\n# Load the model if it exists\nif os.path.exists(model_path):\n    print(f\"Loading model from {model_path}\")\n    model.load_state_dict(torch.load(model_path))\n    model.to(device)\n    val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n\nprint(\"Training model...\")\n\nfor epoch in range(3):\n    print(f'\\nEpoch {epoch+1}/3')\n    train_loss, epoch_time = train_epoch(model, train_loader, optimizer, criterion, device)\n    total_train_time += epoch_time\n\n    # Evaluate on training set\n    print(\"\\nEvaluating on training set:\")\n    train_loss_e, train_accuracy = evaluate(model, train_loader, criterion, device)\n\n    # Evaluate on validation set\n    print(\"\\nEvaluating on validation set:\")\n    val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n\n    # Record metrics\n    train_losses.append(train_loss_e)\n    train_accuracies.append(train_accuracy)\n    val_losses.append(val_loss)\n    val_accuracies.append(val_accuracy)\n    epochs_list.append(epoch + 1)\n\n    # Epoch summary\n    print(f'\\nEpoch Summary:')\n    print(f'Train Loss: {train_loss_e:.4f}, Train Accuracy: {train_accuracy:.4f}')\n    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n    print(f'Epoch Time: {epoch_time:.2f}s')\n    print(f'Total Training Time: {total_train_time/60:.2f} minutes')\n\n    # Step the scheduler based on validation loss\n    scheduler.step(val_loss)\n\n    # Early Stopping based on validation loss\n    if val_loss < best_loss:\n        best_loss = val_loss\n        patience_counter = 0\n        torch.save(model.state_dict(), model_path)\n        print(f\"Model saved to {model_path}\")\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(\"Early stopping triggered.\")\n            break\n\n    # Check for maximum training time\n    if total_train_time >= max_training_time:\n        print(f\"Maximum training time of {max_training_time/3600} hours reached. Stopping training.\")\n        break\n\nprint(\"Training complete!\")\n\n# Save training metrics to disk\nmetrics = {\n    'train_losses': train_losses,\n    'train_accuracies': train_accuracies,\n    'val_losses': val_losses,\n    'val_accuracies': val_accuracies,\n    'epochs': epochs_list,\n}\n\nmetrics_path = '/kaggle/working/training_metrics.pkl'\nwith open(metrics_path, 'wb') as f:\n    pickle.dump(metrics, f)\nprint(f\"Training metrics saved to {metrics_path}\")","metadata":{"execution":{"iopub.status.busy":"2025-02-12T03:03:23.212439Z","iopub.execute_input":"2025-02-12T03:03:23.212742Z","iopub.status.idle":"2025-02-12T03:03:32.670822Z","shell.execute_reply.started":"2025-02-12T03:03:23.212717Z","shell.execute_reply":"2025-02-12T03:03:32.669764Z"},"papermill":{"duration":7373.837123,"end_time":"2025-01-18T17:20:39.514103","exception":false,"start_time":"2025-01-18T15:17:45.676980","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 11. Evaluate the Model\nLoad the best saved model and evaluate its performance on the test set.","metadata":{"papermill":{"duration":0.055249,"end_time":"2025-01-18T17:20:39.626787","exception":false,"start_time":"2025-01-18T17:20:39.571538","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Load the best model\nmodel.load_state_dict(torch.load(model_path))\nmodel.to(device)\n\n# Evaluate on the test set and get predictions\ntest_loss, test_accuracy, test_preds, test_labels = evaluate(\n    model, test_loader, criterion, device, return_predictions=True\n)\nprint(f'Final Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n\n# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ncm = confusion_matrix(test_labels, test_preds)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Not Sarcastic', 'Sarcastic'])\ndisp.plot(cmap=plt.cm.Blues)\nplt.title('Confusion Matrix - Test Set')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n# Plot ROC Curve (Optional)\nfrom sklearn.metrics import roc_curve, auc\n\n# Compute prediction probabilities\ndef get_probs(model, test_loader, device):\n    model.eval()\n    probs = []\n    labels_list = []\n\n    with torch.no_grad():\n        for batch in test_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            raw_texts = batch['raw_text']\n\n            outputs = model(input_ids, attention_mask, raw_texts)\n            probabilities = outputs[:, 1].cpu().numpy()  # Assuming class 1 is 'Sarcastic'\n\n            probs.extend(probabilities)\n            labels_list.extend(labels.cpu().numpy())\n\n    return np.array(probs), np.array(labels_list)\n\n# Get probabilities and labels\nprobs, labels = get_probs(model, test_loader, device)\n\n# Compute ROC curve and AUC\nfpr, tpr, thresholds = roc_curve(labels, probs)\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC curve\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC Curve (area = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic - Test Set')\nplt.legend(loc='lower right')\nplt.show()\n# ... (previous ROC curve code)\n\n\n\n\n\n\n\n# Plot Training Metrics\n# Loss\nplt.figure(figsize=(10, 5))\nplt.plot(epochs_list, train_losses, label='Training Loss')\nplt.plot(epochs_list, val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss vs. Epoch')\nplt.legend()\nplt.show()\n\n# Accuracy\nplt.figure(figsize=(10, 5))\nplt.plot(epochs_list, train_accuracies, label='Training Accuracy')\nplt.plot(epochs_list, val_accuracies, label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Accuracy vs. Epoch')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2025-01-18T17:20:39.738496Z","iopub.status.busy":"2025-01-18T17:20:39.738236Z","iopub.status.idle":"2025-01-18T17:21:05.936863Z","shell.execute_reply":"2025-01-18T17:21:05.936187Z"},"papermill":{"duration":26.256232,"end_time":"2025-01-18T17:21:05.938266","exception":false,"start_time":"2025-01-18T17:20:39.682034","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 12. Conclusion\nIn this notebook, we built a sarcasm detection model that effectively combines textual data processed by BERT and emoji embeddings. We implemented attention mechanisms, residual connections, and used focal loss to handle class imbalance. The model was trained using a cosine annealing scheduler and gradient clipping to improve performance.\n\n","metadata":{"papermill":{"duration":0.058377,"end_time":"2025-01-18T17:21:06.057736","exception":false,"start_time":"2025-01-18T17:21:05.999359","status":"completed"},"tags":[]}},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.058839,"end_time":"2025-01-18T17:21:06.176003","exception":false,"start_time":"2025-01-18T17:21:06.117164","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.058835,"end_time":"2025-01-18T17:21:06.294453","exception":false,"start_time":"2025-01-18T17:21:06.235618","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.059217,"end_time":"2025-01-18T17:21:06.413158","exception":false,"start_time":"2025-01-18T17:21:06.353941","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}